{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_NAME = 'movies_vecdb'\n",
    "DIMENSION = 384\n",
    "URI = 'http://localhost:19530'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores.milvus import Milvus\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "docs = []\n",
    "\n",
    "for file in os.listdir('./movie_text_data/'):\n",
    "    loader = TextLoader(f'./movie_text_data/{file}')\n",
    "    docs.extend(loader.load())\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=0)  # DA OTTIMIZZARE\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "model_name = \"sentence-transformers/all-MiniLM-L12-v2\"\n",
    "model_kwargs = {'device': device}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = Milvus(\n",
    "    embedding_function=embeddings,\n",
    "    connection_args={'host':'127.0.0.1', 'port':'19530'},\n",
    "    drop_old=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = Milvus.from_documents(\n",
    "    all_splits,\n",
    "    embeddings,\n",
    "    connection_args={'host':'127.0.0.1', 'port':'19530'},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Which are the Crime movies directed by Martin Scorsese?\"\n",
    "result = vector_db.similarity_search(query, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCUMENT 1:\n",
      "IMDB ID -> 0101540\n",
      "TITLE -> Cape Fear\n",
      "DIRECTED BY -> Martin Scorsese\n",
      "GENRES -> Crime, Thriller\n",
      "PLOT -> A convicted rapist, released from prison after serving a fourteen-year sentence, stalks the family of the lawyer who originally defended him.\n",
      "YEAR -> 1991\n",
      "STARS -> Robert De Niro, Nick Nolte, Jessica Lange\n",
      "\n",
      "DOCUMENT 2:\n",
      "IMDB ID -> 0031014\n",
      "TITLE -> 6,000 Enemies\n",
      "DIRECTED BY -> George B. Seitz\n",
      "GENRES -> Crime, Drama, Romance\n",
      "PLOT -> A tough prosecutor who has sent dozens of criminals to prison finds himself framed on a bribery charge and winds up in prison himself.\n",
      "YEAR -> 1939\n",
      "STARS -> Walter Pidgeon, Rita Johnson, Paul Kelly\n",
      "\n",
      "DOCUMENT 3:\n",
      "IMDB ID -> 0031014\n",
      "TITLE -> 6,000 Enemies\n",
      "DIRECTED BY -> George B. Seitz\n",
      "GENRES -> Crime, Drama, Romance\n",
      "PLOT -> A tough prosecutor who has sent dozens of criminals to prison finds himself framed on a bribery charge and winds up in prison himself.\n",
      "YEAR -> 1939\n",
      "STARS -> Walter Pidgeon, Rita Johnson, Paul Kelly\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n, doc in enumerate(result):\n",
    "    print(f\"DOCUMENT {n+1}:\")\n",
    "    print(doc.page_content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default collection name - LangChainCollection\n",
      "Default search params - {'metric_type': 'L2', 'params': {'ef': 10}}\n",
      "Default index params - None\n"
     ]
    }
   ],
   "source": [
    "print(f\"Default collection name - {vector_db.collection_name}\")\n",
    "print(f\"Default search params - {vector_db.search_params}\")\n",
    "print(f\"Default index params - {vector_db.index_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Collection>:\n",
       "-------------\n",
       "<name>: LangChainCollection\n",
       "<description>: \n",
       "<schema>: {'auto_id': True, 'description': '', 'fields': [{'name': 'source', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 65535}}, {'name': 'text', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 65535}}, {'name': 'pk', 'description': '', 'type': <DataType.INT64: 5>, 'is_primary': True, 'auto_id': True}, {'name': 'vector', 'description': '', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 384}}], 'enable_dynamic_field': False}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_db.col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ./models/Meta-Llama-3-8B-Instruct-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 128001\n",
      "llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q6_K:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 6.14 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   410.98 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  5871.99 MiB\n",
      ".........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =     9.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\", 'tokenizer.ggml.eos_token_id': '128001', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'llama.context_length': '8192', 'general.name': 'Meta-Llama-3-8B-Instruct', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.file_type': '18', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128'}\n",
      "Using gguf chat template: {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}\n",
      "Using chat eos_token: <|end_of_text|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCpp(\n",
    "        model_path=\"./models/Meta-Llama-3-8B-Instruct-Q6_K.gguf\",\n",
    "        n_gpu_layers=-1,\n",
    "        n_batch=1024,\n",
    "        temperature=0.2,\n",
    "        n_threads=8,\n",
    "        # max_tokens=16384,\n",
    "        # f16_kv=True,\n",
    "        # callback_manager=CallbackManager(StreamingStdOutCallbackHandler()),\n",
    "        verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"<|begin_of_text|>\n",
    "<|start_header_id>system<|end_header_id>\n",
    "You are an helpful AI assistant, which is expert about the field of Movies and Cinema.\n",
    "You have to answer to the question in a precise way: you MUST use the search results to build the answer.\n",
    "If the asked informations don't exist within the search results, DON'T INVENT THEM AND JUST ANSWER 'I don't know how to answer to this question'. \n",
    "\n",
    "Generate some movies and, for each movie, report these informations following this structure:\n",
    "* Title: report here the title of the movie\n",
    "* Year: report here the year of the movie\n",
    "* Genre: report here the list of genres for the movie\n",
    "* Director: report here the director \n",
    "* Brief description of the plot: report here a brief description of the plot\n",
    "* Main actors: report here 2 or 3 members of the cast\n",
    "<|eot_id|>\n",
    "\n",
    "<|start_header_id>user<|end_header_id>\n",
    "Question: {question}\n",
    "<|eot_id|>\n",
    "\n",
    "<start_header_id>assistant<|end_header_id>\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "retriever = vector_db.as_retriever()\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     393.71 ms\n",
      "llama_print_timings:      sample time =     296.69 ms /   205 runs   (    1.45 ms per token,   690.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     293.55 ms /    58 tokens (    5.06 ms per token,   197.58 tokens per second)\n",
      "llama_print_timings:        eval time =    5087.63 ms /   204 runs   (   24.94 ms per token,    40.10 tokens per second)\n",
      "llama_print_timings:       total time =    7209.53 ms /   262 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here are some movies that correspond to at least one of the listed features:\n",
      "\n",
      "1. **Tortilla Soup** (2001) - Directed by Pedro Almodovar's protégé, Robert Rodriguez. The movie is a remake of Almodovar's **Like Water for Chocolate**.\n",
      "2. **We Were Soldiers** (2002) - The plot is related to the Vietnam War.\n",
      "3. **The Thin Red Line** (1998) - Although not directly about the Vietnam War, it is set during World War II and explores themes of war and humanity.\n",
      "4. **Se7en** (1995) - Directed by David Fincher, who has worked with Brad Pitt on several projects.\n",
      "\n",
      "I chose these movies because they all have connections to at least one of the listed features. For example, **Tortilla Soup** is a remake of Pedro Almodovar's movie, and **We Were Soldiers** is a war drama that explores themes related to the Vietnam War.\n"
     ]
    }
   ],
   "source": [
    "request = \"\"\"Report me some movies which correspond to at least one of these features: the movie directed by Pedro Almodovar, the plot is related to the Vietnam War, the movie is less recent than 1988, Brad Pitt is part of the cast.\n",
    "Give more importance to a movie if it corresponds to more than one of the listed features.\n",
    "After reporting the movies, explain why you chose them.\"\"\"\n",
    "\n",
    "resp = rag_chain.invoke(request)\n",
    "print(str(resp))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
