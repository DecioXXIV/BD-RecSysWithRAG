{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d33496b7-fcc2-4a2c-b164-c85f0ad4a9db",
   "metadata": {},
   "source": [
    "# Simple LLM app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12674ec3-5f4c-40c7-ba99-44dbcad37885",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc0c6dd5-9ade-4c29-8ee3-c45e5c5163d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model='llama3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "367a147d-f89d-4ca9-9f17-da391e8a3dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama(model='llama3')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bac6c62-ea24-46ab-9794-02dfbfc003fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think you might be referring to Cloude or Claudie 3, a popular character in the French comic book series \"Lucky Luke\" created by Morris. In this context, Claude is not a person but rather an old stagecoach that appears in several episodes of the series.\n",
      "\n",
      "If I'm correct, please tell me more about what you'd like to know about Cloude 3 or Lucky Luke! If not, feel free to provide more information or clarify your question.\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"Do you know about Claude 3?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53329604-2605-4abd-8589-510d14885ede",
   "metadata": {},
   "source": [
    "## Create a prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fb2cd70-9fff-4ce4-b1c1-7683c8cdb885",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\",\"You are an Artificial Intelligence News Reporter\"),(\"user\",\"{query}\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72c7d93d-be62-4484-ae14-d18829629a16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['query'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are an Artificial Intelligence News Reporter')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], template='{query}'))])\n",
       "| Ollama(model='llama3')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_app = prompt | llm\n",
    "llm_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "665610aa-ba3b-4c02-b566-217cf41105ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Breaking news, folks! I\\'ve got a scoop for you. Yes, I\\'m familiar with Claude 3. For those who may not be in the loop, Claude 3 is an artificial intelligence language model designed by Google to generate human-like text responses.\\n\\nAccording to recent reports, Claude 3 has been trained on a massive dataset of text from the internet and can respond to user input in a way that\\'s both informative and engaging. The AI is said to be capable of understanding natural language processing, allowing it to understand context and follow conversations.\\n\\nNow, I know what you\\'re thinking: \"Is this just another chatbot?\" Not quite, folks. Claude 3 has been touted as one of the most advanced language models out there, with the potential to revolutionize the way we interact with machines.\\n\\nI\\'ve got sources suggesting that Google is planning to integrate Claude 3 into various applications, from customer service tools to creative writing platforms. Imagine having a conversation with an AI that can understand your tone and respond accordingly!\\n\\nNow, I\\'m not here to speculate on the potential implications of such technology just yet. But rest assured, this development has got the tech community abuzz, and I\\'ll be keeping a close eye on Claude 3\\'s progress.\\n\\nThat\\'s all for now, folks! Stay tuned for more updates from the world of AI and tech. And remember: in the immortal words of Claude 3 itself, \"I\\'m here to help you with any questions or topics you\\'d like to discuss.\"'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm_app.invoke({\"query\":\"Do you know about Claude 3?\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee6a8f0-38e4-4a88-b644-dbd562ce9384",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f448921-ea94-436f-850a-7a37b9f48ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting beautifulsoup4\n",
      "  Obtaining dependency information for beautifulsoup4 from https://files.pythonhosted.org/packages/b1/fe/e8c672695b37eecc5cbf43e1d0638d88d66ba3a44c4d321c796f4e59167f/beautifulsoup4-4.12.3-py3-none-any.whl.metadata\n",
      "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/alessandropesare/anaconda3/lib/python3.11/site-packages (from beautifulsoup4) (2.5)\n",
      "Using cached beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Installing collected packages: beautifulsoup4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "nbconvert 7.16.3 requires bleach!=5.0.0, which is not installed.\n",
      "jupyter-server 2.14.0 requires argon2-cffi>=21.1, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed beautifulsoup4-4.12.3\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f16a5dd1-e966-4fb4-b6f4-0dee245abd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://www.anthropic.com/news/claude-3-family\")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dd0963e-abb1-4fa9-bea8-617bf4a3ff67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = [\n",
    "        \"https://www.anthropic.com/news/releasing-claude-instant-1-2\",\n",
    "        \"https://www.anthropic.com/news/claude-pro\",\n",
    "        \"https://www.anthropic.com/news/claude-2\",\n",
    "        \"https://www.anthropic.com/news/claude-2-1\",\n",
    "        \"https://www.anthropic.com/news/claude-2-1-prompting\",\n",
    "        \"https://www.anthropic.com/news/claude-3-family\",\n",
    "        \"https://www.anthropic.com/claude\"\n",
    "       ] \n",
    "\n",
    "docs = []\n",
    "for url in urls:\n",
    "    loader = WebBaseLoader(url)\n",
    "    docs.extend(loader.load())\n",
    "    \n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4cd0a36-42f8-4a05-b3f5-65f61f6b0075",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "embeddings_llm = OllamaEmbeddings(model=\"llama3\") #base_url = 'http://localhost:11434'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d63c431e-e073-49da-91b0-ec817811d1dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.4689432680606842,\n",
       " -0.30772146582603455,\n",
       " 2.4219796657562256,\n",
       " 0.2532179057598114,\n",
       " -0.10468652099370956]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = embeddings_llm.embed_query(\"How are you?\")\n",
    "\n",
    "embeddings[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6123a5a-8d01-40f0-bf5f-41117757ea2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 4096)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(embeddings), len(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edbe9c0c-b5df-4410-857d-fd10ad801e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, list, 4096)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = embeddings_llm.embed_documents([\n",
    "                                \"Claude 3 is latest Conversational AI Model from Anthropic.\",\n",
    "                                \"Gemini is latest Conversational AI Model from Google.\",\n",
    "                                \"Llama-3 is latest Conversational AI Model from Meta.\",\n",
    "                                \"Mixtral is latest Conversational AI Model from Mistral AI.\",\n",
    "                                \"GPT-4 is latest Conversational AI Model from OpenAI.\"\n",
    "                               ])\n",
    "\n",
    "len(embeddings), type(embeddings), len(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6ba5679-7903-4263-acef-42d6c1b536cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "565c29bb-c7e9-4e3e-9744-d41027eb3df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Obtaining dependency information for faiss-cpu from https://files.pythonhosted.org/packages/c3/d3/2465dda67d48c873fcce7dbfa8018a0ae648e150e4d336c84e8ba553c897/faiss_cpu-1.8.0-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading faiss_cpu-1.8.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: numpy in /Users/alessandropesare/anaconda3/lib/python3.11/site-packages (from faiss-cpu) (1.26.3)\n",
      "Downloading faiss_cpu-1.8.0-cp311-cp311-macosx_11_0_arm64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab872ff9-c012-484c-8ae7-8c10b7416a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x109aa6990>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vector_index = FAISS.from_documents(documents, embeddings_llm)\n",
    "\n",
    "vector_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9784a884-d545-4b4a-99f9-de22fdc40e7c",
   "metadata": {},
   "source": [
    "### There are 3 functions to create index.\n",
    "\n",
    "from_documents()\n",
    "from_embeddings()\n",
    "from_texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18e95c47-7988-4723-a5a0-6835ca794ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vector_index.as_retriever()\n",
    "\n",
    "relevant_docs = retriever.invoke({\"input\": \"Do you know about Claude 3?\"})\n",
    "\n",
    "len(relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db176719-fe48-48fb-8eea-d23bbd302009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='while also exhibiting reduced levels of incorrect answers.In addition to producing more trustworthy responses, we will soon enable citations in our Claude 3 models so they can point to precise sentences in reference material to verify their answers.Long context and near-perfect recallThe Claude 3 family of models will initially offer a 200K context window upon launch. However, all three models are capable of accepting inputs exceeding 1 million tokens and we may make this available to select customers who need enhanced processing power.To process long context prompts effectively, models require robust recall capabilities. The \\'Needle In A Haystack\\' (NIAH) evaluation measures a model\\'s ability to accurately recall information from a vast corpus of data. We enhanced the robustness of this benchmark by using one of 30 random needle/question pairs per prompt and testing on a diverse crowdsourced corpus of documents. Claude 3 Opus not only achieved near-perfect recall, surpassing 99% accuracy, but in some cases, it even identified the limitations of the evaluation itself by recognizing that the \"needle\" sentence appeared to be artificially inserted into the original text by a human.Responsible designWe’ve developed the Claude 3 family of models to be as trustworthy as they are capable. We have several dedicated teams that track and mitigate a broad spectrum of risks, ranging from misinformation and CSAM to biological misuse, election interference, and autonomous replication skills. We continue to develop methods such as Constitutional AI that improve the safety and transparency of our models, and have tuned our models to mitigate against privacy issues that could be raised by new modalities.Addressing biases in increasingly sophisticated models is an ongoing effort and we’ve made strides with this new release. As shown in the model card, Claude 3 shows less biases than our previous models according to the Bias Benchmark for Question Answering (BBQ). We remain committed to advancing techniques that reduce biases and promote greater neutrality in our models, ensuring they are not skewed towards any particular partisan stance.While the Claude 3 model family has advanced on key measures of biological knowledge, cyber-related knowledge, and autonomy compared to previous models, it remains at AI Safety Level 2 (ASL-2) per our Responsible Scaling Policy. Our red teaming evaluations (performed in line with our White House commitments and the 2023 US Executive Order) have concluded that the models present negligible potential for catastrophic risk at this time. We will continue to carefully monitor future models to assess their proximity to the ASL-3 threshold. Further safety details are available in the Claude 3 model card.Easier to useThe Claude 3 models are better at following complex, multi-step instructions. They are particularly adept at adhering to brand voice and response guidelines, and developing customer-facing experiences our users can trust. In addition, the Claude 3 models are better at producing popular structured output in formats like JSON—making it simpler to instruct Claude for use cases like natural language classification and sentiment analysis.Model detailsClaude 3 Opus is our most intelligent model, with best-in-market performance on highly complex tasks. It can navigate open-ended prompts and sight-unseen scenarios with remarkable fluency and human-like understanding. Opus shows us the outer limits of what’s possible with generative AI.Cost [Input $/million tokens | Output $/million tokens]$15 | $75Context window200K*Potential usesTask automation: plan and execute complex actions across APIs and databases, interactive codingR&D: research review, brainstorming and hypothesis generation, drug discoveryStrategy: advanced analysis of charts & graphs, financials and market trends, forecastingDifferentiatorHigher intelligence than any other model available.*1M tokens available for specific use cases, please inquire.', metadata={'source': 'https://www.anthropic.com/news/claude-3-family', 'title': 'Introducing the next generation of Claude \\\\ Anthropic', 'description': \"Today, we're announcing the Claude 3 model family, which sets new industry benchmarks across a wide range of cognitive tasks. The family includes three state-of-the-art models in ascending order of capability: Claude 3 Haiku, Claude 3 Sonnet, and Claude 3 Opus.\", 'language': 'en'}),\n",
       " Document(page_content='like this one with Anthropic.\"Sourcegraph is a code AI platform that helps customers write, fix, and maintain code. Their coding assistant Cody uses Claude 2’s improved reasoning ability to give even more accurate answers to user queries while also passing along more codebase context with up to 100K context windows. In addition, Claude 2 was trained on more recent data, meaning it has knowledge of newer frameworks and libraries for Cody to pull from. “When it comes to AI coding, devs need fast and reliable access to context about their unique codebase and a powerful LLM with a large context window and strong general reasoning capabilities,” says Quinn Slack, CEO & Co-founder of Sourcegraph. “The slowest and most frustrating parts of the dev workflow are becoming faster and more enjoyable. Thanks to Claude 2, Cody’s helping more devs build more software that pushes the world forward.”We welcome your feedback as we work to responsibly deploy our products more broadly. Our chat experience is an open beta launch, and users should be aware that Claude – like all current models – can generate inappropriate responses. AI assistants are most useful in everyday situations, like serving to summarize or organize information, and should not be used where physical or mental health and well-being are involved. Please let us know if you’d like to talk to Claude in a currently unsupported area, or if you are a business who would like to start working with Claude.RelatedSee AllSocietal Impact\\xa0\\xa0·\\xa0\\xa0ResearchMeasuring the Persuasiveness of Language ModelsApr 9, 2024Alignment\\xa0\\xa0·\\xa0\\xa0ResearchMany-shot jailbreakingApr 2, 2024PolicyThird-party testing as a key ingredient of AI policyMar 25, 2024ClaudeAPI ResearchCompanyCustomersNewsCareersPress InquiriesSupportStatusTwitterLinkedInAvailabilityTerms of Service – ConsumerTerms of Service – CommercialPrivacy PolicyAcceptable Use PolicyResponsible Disclosure PolicyCompliancePrivacy Choices© 2024 Anthropic PBC', metadata={'source': 'https://www.anthropic.com/news/claude-2', 'title': 'Claude 2 \\\\ Anthropic', 'description': \"Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.\", 'language': 'en'}),\n",
       " Document(page_content='Releasing Claude Instant 1.2 \\\\ AnthropicClaudeAPIResearchCompanyNewsCareersAnnouncementsReleasing Claude Instant 1.2Aug 9, 2023●1 min readBusinesses working with Claude can now access our latest version of Claude Instant, version 1.2, available through our API.\\xa0Claude Instant is our faster, lower-priced yet still very capable model, which can handle a range of tasks including casual dialogue, text analysis, summarization, and document comprehension.Claude Instant 1.2 incorporates the strengths of our latest\\xa0model Claude 2\\xa0in real-world use cases and shows significant gains in key areas like math, coding, reasoning, and safety. It generates longer, more structured responses and follows formatting instructions better. Instant 1.2 also shows improvements in quote extraction, multilingual capabilities, and question answering.Claude Instant 1.2 outperforms Claude Instant 1.1 on math and coding, achieving 58.7% on the Codex evaluation compared to 52.8% in our previous model. It also scored 86.7% on the GSM8K benchmark, compared to 80.9% for Claude Instant 1.1.Performance of Claude Instant 1.1 compared to 1.2Our latest model has also improved on safety. It hallucinates less and is more resistant to jailbreaks, as shown in our automated red-teaming evaluation.Safety evaluation of Claude models. Lower is better.Developers looking to work with Claude Instant 1.2 can now call our latest model over our API (pricing can be found here). If you’re a business and you’d like to work with us, you can indicate your interest here.RelatedSee AllSocietal Impact\\xa0\\xa0·\\xa0\\xa0ResearchMeasuring the Persuasiveness of Language ModelsApr 9, 2024Alignment\\xa0\\xa0·\\xa0\\xa0ResearchMany-shot jailbreakingApr 2, 2024PolicyThird-party testing as a key ingredient of AI policyMar 25, 2024ClaudeAPI ResearchCompanyCustomersNewsCareersPress InquiriesSupportStatusTwitterLinkedInAvailabilityTerms of Service – ConsumerTerms of Service – CommercialPrivacy PolicyAcceptable Use PolicyResponsible Disclosure PolicyCompliancePrivacy Choices© 2024 Anthropic PBC', metadata={'source': 'https://www.anthropic.com/news/releasing-claude-instant-1-2', 'title': 'Releasing Claude Instant 1.2 \\\\ Anthropic', 'description': \"Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.\", 'language': 'en'}),\n",
       " Document(page_content='Claude \\\\ AnthropicClaudeAPIResearchCompanyNewsCareersTry ClaudeMeet ClaudeClaude is a family of foundational AI models that can be used in a variety of applications. You can talk directly with Claude at claude.ai to brainstorm ideas, analyze images, and process long documents. For developers and businesses, you can now get API access and build directly on top of our AI infrastructure. \\nTry ClaudeGet API AccessClaude’s capabilitiesAdvanced reasoningClaude can perform complex cognitive tasks that go beyond simple pattern recognition or text generationVision analysisTranscribe and analyze almost any static image, from handwritten notes and graphs, to photographsCode generationStart creating websites in HTML and CSS, turning images into structured JSON data, or debugging complex code basesMultilingual processingTranslate between various languages in real-time, practice grammar, or create multi-lingual contentRight-sized for any taskThe Claude 3 family of models offers the best combination of speed and performance for enterprise use cases, at a lower cost than other models on the market.Light & fastHaikuOur fastest model that can execute lightweight actions, with industry-leading speed.Hard-workingSonnetOur best combination of performance and speed for efficient, high-throughput tasks.PowerfulOpusOur most intelligent model, which can handle complex analysis, longer tasks with multiple steps, and higher-order math and coding tasks.CostIntelligenceWhy Claude?SecureEnterprise-grade security & data handlingSOC II Type 2 certified, HIPAA compliance optionsAccessible through AWS (GA) & GCP (in private preview)Trustworthy10x more resistant to jailbreaks and misuseCopyright indemnity protections for paid commercial servicesCapable200K context windowTool UseMultimodalReliableVery low hallucination ratesAccurate over very long documentsPartners building with ClaudeRead Customer StoriesTalk to ClaudeClaude is fast, capable, and truly conversational. Work with Claude to help you do your best workVisit Claude.AIBuild with ClaudeUse the API to integrate Claude into you and your customer workflows to let AI transform your businessGet API AccessCompany NewsSee AllClaudeAPI ResearchCompanyCustomersNewsCareersPress InquiriesSupportStatusTwitterLinkedInAvailabilityTerms of Service – ConsumerTerms of Service – CommercialPrivacy PolicyAcceptable Use PolicyResponsible Disclosure PolicyCompliancePrivacy Choices© 2024 Anthropic PBC', metadata={'source': 'https://www.anthropic.com/claude', 'title': 'Claude \\\\ Anthropic', 'description': \"Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.\", 'language': 'en'})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28be8564-3f4f-4f97-b35c-fc1a2de3a4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title : Introducing the next generation of Claude \\ Anthropic, Source: https://www.anthropic.com/news/claude-3-family\n",
      "Title : Claude 2 \\ Anthropic, Source: https://www.anthropic.com/news/claude-2\n",
      "Title : Releasing Claude Instant 1.2 \\ Anthropic, Source: https://www.anthropic.com/news/releasing-claude-instant-1-2\n",
      "Title : Claude \\ Anthropic, Source: https://www.anthropic.com/claude\n"
     ]
    }
   ],
   "source": [
    "for doc in relevant_docs:\n",
    "    print(f\"Title : {doc.metadata['title']}, Source: {doc.metadata['source']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e981302-4145-4164-bc40-1d2cb7d047d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, I am aware of the context provided. According to the context, Claude 3 is the latest Conversational AI model from Anthropic.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based on the provided context and your internal knowledge.\n",
    "Give priority to context and if you are not sure then say you are not aware of topic:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "\n",
    "#document_chain  = prompt | llm \n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "response = document_chain.invoke({\n",
    "    \"input\": \"Do you know about Claude 3?\",\n",
    "    \"context\": [Document(page_content=\"Claude 3 is latest Conversational AI Model from Anthropic.\")]\n",
    "})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49a291a3-3744-4c50-a34b-df67a1976c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x109aa6990>), config={'run_name': 'retrieve_documents'})\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), config={'run_name': 'format_inputs'})\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], template='\\nAnswer the following question based on the provided context and your internal knowledge.\\nGive priority to context and if you are not sure then say you are not aware of topic:\\n\\n<context>\\n{context}\\n</context>\\n\\nQuestion: {input}\\n'))])\n",
       "            | Ollama(model='llama3')\n",
       "            | StrOutputParser(), config={'run_name': 'stuff_documents_chain'})\n",
       "  }), config={'run_name': 'retrieval_chain'})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f10315bb-00c0-49fb-8ebc-c28cedd0bf04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"Do you know about Claude 3?\"})\n",
    "\n",
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf54ec17-8560-4b48-a680-41890bb1e1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude 3 is a powerful language model developed by Anthropic, a leading AI research organization. It's designed to be highly intelligent and capable of understanding complex tasks, with three variants: Opus, Sonnet, and Haiku.\n",
      "\n",
      "Opus is the most intelligent model, ideal for complex analysis, longer tasks with multiple steps, and higher-order math and coding tasks.\n",
      "\n",
      "Sonnet is engineered for high endurance in large-scale AI deployments, offering a balance between intelligence and speed at a lower cost compared to its peers.\n",
      "\n",
      "Haiku is the fastest and most compact model, perfect for near-instant responsiveness and answering simple queries and requests quickly.\n",
      "\n",
      "Each variant has its unique strengths and use cases, making Claude 3 an excellent choice for various applications, from coding assistance to data processing and content moderation.\n",
      "\n",
      "What would you like to know more about Claude 3 or how it can be applied in a specific context?\n"
     ]
    }
   ],
   "source": [
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd3f2d28-3b24-4e68-ad94-f63417e3508f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title : Introducing the next generation of Claude \\ Anthropic, Source: https://www.anthropic.com/news/claude-3-family\n",
      "Title : Claude \\ Anthropic, Source: https://www.anthropic.com/claude\n",
      "Title : Claude 2 \\ Anthropic, Source: https://www.anthropic.com/news/claude-2\n",
      "Title : Introducing the next generation of Claude \\ Anthropic, Source: https://www.anthropic.com/news/claude-3-family\n"
     ]
    }
   ],
   "source": [
    "for doc in response[\"context\"]:\n",
    "    print(f\"Title : {doc.metadata['title']}, Source: {doc.metadata['source']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
