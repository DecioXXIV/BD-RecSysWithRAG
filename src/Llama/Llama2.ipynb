{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bKQIsIq-d8y"
   },
   "source": [
    "**Llama 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnV5UC7A2vBZ"
   },
   "source": [
    "The Llama 2 is a collection of pretrained and fine-tuned generative text models, ranging from 7 billion to 70 billion parameters, designed for dialogue use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AC41zK5l3Abp"
   },
   "source": [
    " It outperforms open-source chat models on most benchmarks and is on par with popular closed-source models in human evaluations for helpfulness and safety."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4nobX9E83PjQ"
   },
   "source": [
    "[Llama 2 13B-chat](https://huggingface.co/meta-llama/Llama-2-13b-chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0K4QuEDH4CbY"
   },
   "source": [
    "`llama.cpp`'s objective is to run the LLaMA model with 4-bit integer quantization on MacBook. It is a plain C/C++ implementation optimized for Apple silicon and x86 architectures, supporting various integer quantization and BLAS libraries. Originally a web chat example, it now serves as a development playground for ggml library features.\n",
    "\n",
    "`GGML`, a C library for machine learning, facilitates the distribution of large language models (LLMs). It utilizes quantization to enable efficient LLM execution on consumer hardware. GGML files contain binary-encoded data, including version number, hyperparameters, vocabulary, and weights. The vocabulary comprises tokens for language generation, while the weights determine the LLM's size. Quantization reduces precision to optimize resource usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3YC846SH5DOK"
   },
   "source": [
    "#  Quantized Models from the Hugging Face Community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0TD82wis5LGA"
   },
   "source": [
    "The Hugging Face community provides quantized models, which allow us to efficiently and effectively utilize the model on the T4 GPU. It is important to consult reliable sources before using any model.\n",
    "\n",
    "There are several variations available, but the ones that interest us are based on the GGLM library.\n",
    "\n",
    "We can see the different variations that Llama-2-13B-GGML has [here](https://huggingface.co/models?search=llama%202%20ggml).\n",
    "\n",
    "\n",
    "\n",
    "In this case, we will use the model called [Llama-2-13B-chat-GGML](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQZBmz7I5neU"
   },
   "source": [
    "#**Step 1: Install All the Required Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L0avf7xx2lcj"
   },
   "outputs": [],
   "source": [
    "# GPU llama-cpp-python\n",
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78 numpy==1.23.4 --force-reinstall --upgrade --no-cache-dir --verbose\n",
    "!pip install huggingface_hub\n",
    "!pip install llama-cpp-python==0.1.78\n",
    "!pip install numpy==1.23.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qJ90LnMv54Y-"
   },
   "outputs": [],
   "source": [
    "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
    "model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\" # the model is in bin format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6lOmpKB36RJh"
   },
   "source": [
    "#**Step 2: Import All the Required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Ak3ZtGjM6Wdp"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "85XOzmui6rGN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3060, compute capability 8.6\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "haAb9kNm6J9n"
   },
   "source": [
    "#**Step 3: Download the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173,
     "referenced_widgets": [
      "f4f4b4a026a44b0f9fb2444e2aaba75e",
      "c4e454f8f20a464687c169f7a7a1fe9b",
      "31e741a46bd34f1e9f577c41376e2c08",
      "c2977c3862d845f99af150216ab05ff7",
      "65d7f3826be0482aa702e5c5f2be5eb2",
      "40accdff69474c0193be79d701f3c306",
      "e82c835d917042a7aba27feab6f0ecd8",
      "93e7404bda4246318d831d51d142326d",
      "e1fdf9e9541948d3a274072b73a0b800",
      "c612389940744049a31dd68b7d7ca300",
      "56b6f1132d404f8c822717b62def392d"
     ]
    },
    "id": "qBgdGV4b6MxG",
    "outputId": "75f1afda-3a0a-48da-b03d-8c473316fd08"
   },
   "outputs": [],
   "source": [
    "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQ6OYnI46kKq"
   },
   "source": [
    "#**Step 4: Loading the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "irftToUj6aWt",
    "outputId": "69f805ed-b559-4c7e-8160-81fa2e786ce7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /home/xodiec/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/3140827b4dfcb6b562cd87ee3d7f07109b014dd0/llama-2-13b-chat.ggmlv3.q5_1.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 9 (mostly Q5_1)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.11 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  =  483.30 MB (+  400.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 9954 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "# GPU\n",
    "lcpp_llm = None\n",
    "lcpp_llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_threads=os.cpu_count(), # CPU cores\n",
    "    n_batch=512, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "    n_gpu_layers=-1 # Change this value based on your model and your GPU VRAM pool.\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YG4Pylz662At",
    "outputId": "523e8ffe-a8a4-42a5-bb19-379c26f61228"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2147483647"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the number of layers in GPU\n",
    "lcpp_llm.params.n_gpu_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iE-M307R6_pT"
   },
   "source": [
    "#**Step 5: Create a Prompt Template**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "RfzwELMC7Dyg"
   },
   "outputs": [],
   "source": [
    "prompt = \"Tell me about the best films of 2021\"\n",
    "prompt_template=f'''SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
    "\n",
    "USER: {prompt}\n",
    "\n",
    "ASSISTANT:\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aT8pg6zt7QzA"
   },
   "source": [
    "#**Step 6: Generating the Response**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Parametri LLM:\n",
    "prompt: È il testo di input che fornisci al modello per generare una risposta. In questo caso, prompt_template sarà il modello che fornirà il\n",
    "contesto per la generazione della risposta.\n",
    "max_tokens: Indica il massimo numero di token (parole o caratteri) che il modello può generare come risposta. Qui è impostato su 256 token,\n",
    "il che significa che la risposta avrà al massimo 256 parole o caratteri.\n",
    "temperature: È un parametro che controlla la casualità delle predizioni del modello durante la generazione del testo.\n",
    "Valori più bassi rendono il testo più deterministico, mentre valori più alti aumentano la casualità. Qui è impostato su 0.5, quindi il testo\n",
    "generato sarà moderatamente casuale.\n",
    "top_p: È un'altra tecnica di campionamento che controlla la distribuzione di probabilità delle predizioni del modello.\n",
    "Impostando un valore di top_p alto come 0.95, si assicura che il modello tenga conto delle probabilità cumulative delle parole\n",
    "fino a raggiungere il 95%, limitando le predizioni a un insieme di parole con alta probabilità.\n",
    "repeat_penalty: È un parametro che penalizza il modello quando genera ripetizioni consecutive di parole o frasi. Un valore di 1.2\n",
    "indica una penalità moderata per le ripetizioni.\n",
    "top_k: È un'altra tecnica di campionamento che limita le scelte del modello ai primi k token con la più alta probabilità nella distribuzione\n",
    "di probabilità delle parole. Qui è impostato su 150, il che significa che il modello considererà solo le 150 parole più probabili durante\n",
    "la generazione del testo.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0aF0qWUJ7OPK",
    "outputId": "f7d3a43a-5eed-4a22-c01b-48faf9c8a244"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   808.75 ms\n",
      "llama_print_timings:      sample time =    93.89 ms /   256 runs   (    0.37 ms per token,  2726.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =   808.68 ms /    45 tokens (   17.97 ms per token,    55.65 tokens per second)\n",
      "llama_print_timings:        eval time = 14269.65 ms /   255 runs   (   55.96 ms per token,    17.87 tokens per second)\n",
      "llama_print_timings:       total time = 15612.41 ms\n"
     ]
    }
   ],
   "source": [
    "response=lcpp_llm(prompt=prompt_template, max_tokens=256, temperature=0.5, top_p=0.95,\n",
    "                  repeat_penalty=1.2, top_k=150,\n",
    "                  echo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jlJ1JgR68DDO",
    "outputId": "afdd2a1b-0684-40d7-a49d-d8c482041bfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-0d1010f0-f19d-45e6-9ad3-32c5701f7bb0', 'object': 'text_completion', 'created': 1713270779, 'model': '/home/xodiec/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/3140827b4dfcb6b562cd87ee3d7f07109b014dd0/llama-2-13b-chat.ggmlv3.q5_1.bin', 'choices': [{'text': 'SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\\n\\nUSER: Tell me about the best films of 2021\\n\\nASSISTANT:\\n\\nThere were many great films released in 2021! Here are some that received high praise from critics and audiences alike:\\n\\n1. \"The Power of the Dog\" - A psychological drama directed by Jane Campion, starring Benedict Cumberbatch and Kirsten Dunst. It tells the story of a wealthy family in 1920s Montana and their struggles with love, greed, and power.\\n2. \"The Matrix Resurrections\" - A science fiction action film directed by Lana Wachowski, starring Keanu Reeves, Carrie-Anne Moss, and Yahya Abdul-Mateen II. It is the fourth installment in The Matrix series and follows Neo as he navigates a new reality.\\n3. \"The French Dispatch\" - A comedy-drama film directed by Wes Anderson, set in a fictional French city. It features an all-star cast, including Timothée Chalamet, Benicio del Toro, Adrien Brody, and Tilda Swinton. The story revolves around the staff of a newspaper and their various adventures.\\n4. \"Dune\"', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 45, 'completion_tokens': 256, 'total_tokens': 301}}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qona58gX8oAn",
    "outputId": "9b86d544-79b0-4048-e636-bbb21dccedae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
      "\n",
      "USER: Tell me about the best films of 2021\n",
      "\n",
      "ASSISTANT:\n",
      "\n",
      "There were many great films released in 2021! Here are some that received high praise from critics and audiences alike:\n",
      "\n",
      "1. \"The Power of the Dog\" - A psychological drama directed by Jane Campion, starring Benedict Cumberbatch and Kirsten Dunst. It tells the story of a wealthy family in 1920s Montana and their struggles with love, greed, and power.\n",
      "2. \"The Matrix Resurrections\" - A science fiction action film directed by Lana Wachowski, starring Keanu Reeves, Carrie-Anne Moss, and Yahya Abdul-Mateen II. It is the fourth installment in The Matrix series and follows Neo as he navigates a new reality.\n",
      "3. \"The French Dispatch\" - A comedy-drama film directed by Wes Anderson, set in a fictional French city. It features an all-star cast, including Timothée Chalamet, Benicio del Toro, Adrien Brody, and Tilda Swinton. The story revolves around the staff of a newspaper and their various adventures.\n",
      "4. \"Dune\"\n"
     ]
    }
   ],
   "source": [
    "print(response[\"choices\"][0][\"text\"])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "31e741a46bd34f1e9f577c41376e2c08": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_93e7404bda4246318d831d51d142326d",
      "max": 9763701888,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e1fdf9e9541948d3a274072b73a0b800",
      "value": 9763701888
     }
    },
    "40accdff69474c0193be79d701f3c306": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56b6f1132d404f8c822717b62def392d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "65d7f3826be0482aa702e5c5f2be5eb2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "93e7404bda4246318d831d51d142326d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c2977c3862d845f99af150216ab05ff7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c612389940744049a31dd68b7d7ca300",
      "placeholder": "​",
      "style": "IPY_MODEL_56b6f1132d404f8c822717b62def392d",
      "value": " 9.76G/9.76G [01:17&lt;00:00, 41.0MB/s]"
     }
    },
    "c4e454f8f20a464687c169f7a7a1fe9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_40accdff69474c0193be79d701f3c306",
      "placeholder": "​",
      "style": "IPY_MODEL_e82c835d917042a7aba27feab6f0ecd8",
      "value": "llama-2-13b-chat.ggmlv3.q5_1.bin: 100%"
     }
    },
    "c612389940744049a31dd68b7d7ca300": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e1fdf9e9541948d3a274072b73a0b800": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e82c835d917042a7aba27feab6f0ecd8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f4f4b4a026a44b0f9fb2444e2aaba75e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c4e454f8f20a464687c169f7a7a1fe9b",
       "IPY_MODEL_31e741a46bd34f1e9f577c41376e2c08",
       "IPY_MODEL_c2977c3862d845f99af150216ab05ff7"
      ],
      "layout": "IPY_MODEL_65d7f3826be0482aa702e5c5f2be5eb2"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
